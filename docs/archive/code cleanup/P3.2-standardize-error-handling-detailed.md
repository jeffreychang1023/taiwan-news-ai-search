# P3.2: Standardize Error Handling - Detailed Implementation

**Estimated Effort**: 1 day
**Impact**: Medium - Improves debugging and error consistency
**ROI**: ⭐⭐⭐

---

## Problem Statement

Error handling in the codebase is inconsistent:
- Some methods use bare `except Exception`
- Error messages vary in format
- No standard error hierarchy
- Difficult to trace errors across layers

---

## Solution

1. Define custom exception hierarchy
2. Create error handling decorators
3. Apply consistent error patterns

---

## Implementation

### Step 1: Define Exception Hierarchy

#### File: `code/python/core/exceptions.py`

```python
"""Custom exception hierarchy for NLWeb."""

from typing import Dict, Any, Optional


class NLWebError(Exception):
    """
    Base exception for all NLWeb errors.

    All custom exceptions should inherit from this class.
    """

    def __init__(
        self,
        message: str,
        details: Optional[Dict[str, Any]] = None,
        cause: Optional[Exception] = None,
    ):
        """
        Initialize NLWebError.

        Args:
            message: Human-readable error message
            details: Optional dictionary with error context
            cause: Optional original exception that caused this error
        """
        self.message = message
        self.details = details or {}
        self.cause = cause
        super().__init__(self.message)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for logging/API responses."""
        result = {
            "error_type": self.__class__.__name__,
            "message": self.message,
        }

        if self.details:
            result["details"] = self.details

        if self.cause:
            result["cause"] = str(self.cause)

        return result

    def __str__(self) -> str:
        """String representation with details."""
        base = f"{self.__class__.__name__}: {self.message}"
        if self.details:
            base += f" | Details: {self.details}"
        if self.cause:
            base += f" | Caused by: {self.cause}"
        return base


# Retrieval Errors

class RetrievalError(NLWebError):
    """Error during document retrieval."""
    pass


class VectorSearchError(RetrievalError):
    """Error during vector similarity search."""

    def __init__(
        self,
        message: str,
        query: Optional[str] = None,
        collection: Optional[str] = None,
        **kwargs
    ):
        details = kwargs.pop("details", {})
        if query:
            details["query"] = query
        if collection:
            details["collection"] = collection
        super().__init__(message, details=details, **kwargs)


class BM25SearchError(RetrievalError):
    """Error during BM25 search."""
    pass


# Ranking Errors

class RankingError(NLWebError):
    """Error during result ranking."""
    pass


class LLMRankingError(RankingError):
    """Error during LLM-based ranking."""

    def __init__(
        self,
        message: str,
        model: Optional[str] = None,
        **kwargs
    ):
        details = kwargs.pop("details", {})
        if model:
            details["model"] = model
        super().__init__(message, details=details, **kwargs)


class XGBoostRankingError(RankingError):
    """Error during XGBoost ranking."""
    pass


# Reasoning Errors

class ReasoningError(NLWebError):
    """Error during reasoning/research process."""
    pass


class AnalystError(ReasoningError):
    """Error in analyst agent."""
    pass


class CriticError(ReasoningError):
    """Error in critic agent."""
    pass


class WriterError(ReasoningError):
    """Error in writer agent."""
    pass


class GapEnrichmentError(ReasoningError):
    """Error during gap detection or enrichment."""
    pass


# LLM Errors

class LLMError(NLWebError):
    """Error from LLM API calls."""

    def __init__(
        self,
        message: str,
        model: Optional[str] = None,
        provider: Optional[str] = None,
        status_code: Optional[int] = None,
        **kwargs
    ):
        details = kwargs.pop("details", {})
        if model:
            details["model"] = model
        if provider:
            details["provider"] = provider
        if status_code:
            details["status_code"] = status_code
        super().__init__(message, details=details, **kwargs)


class LLMTimeoutError(LLMError):
    """LLM request timed out."""
    pass


class LLMRateLimitError(LLMError):
    """LLM rate limit exceeded."""
    pass


class LLMResponseParseError(LLMError):
    """Failed to parse LLM response."""

    def __init__(
        self,
        message: str,
        raw_response: Optional[str] = None,
        **kwargs
    ):
        details = kwargs.pop("details", {})
        if raw_response:
            # Truncate for logging
            details["raw_response"] = raw_response[:500] + "..." if len(raw_response) > 500 else raw_response
        super().__init__(message, details=details, **kwargs)


# Validation Errors

class ValidationError(NLWebError):
    """Data validation error."""
    pass


class QueryValidationError(ValidationError):
    """Query validation failed."""
    pass


class ResponseValidationError(ValidationError):
    """Response validation failed."""
    pass


# Configuration Errors

class ConfigurationError(NLWebError):
    """Configuration or setup error."""
    pass


class MissingConfigError(ConfigurationError):
    """Required configuration is missing."""

    def __init__(
        self,
        config_name: str,
        key: Optional[str] = None,
        **kwargs
    ):
        message = f"Missing configuration: {config_name}"
        if key:
            message += f".{key}"
        super().__init__(message, **kwargs)


# Source Errors

class SourceError(NLWebError):
    """Error related to source handling."""
    pass


class NoValidSourcesError(SourceError):
    """No valid sources available after filtering."""

    def __init__(
        self,
        mode: str,
        original_count: int,
        filtered_count: int = 0,
        **kwargs
    ):
        message = f"No valid sources for mode '{mode}'"
        details = {
            "mode": mode,
            "original_count": original_count,
            "filtered_count": filtered_count,
        }
        super().__init__(message, details=details, **kwargs)
```

---

### Step 2: Create Error Handling Decorators

#### File: `code/python/core/error_handling.py`

```python
"""Error handling decorators and utilities."""

from functools import wraps
import logging
from typing import Callable, TypeVar, Any, Optional, Type, Union, List
import asyncio

from .exceptions import (
    NLWebError, LLMError, LLMTimeoutError, LLMRateLimitError,
    ReasoningError, RetrievalError
)

T = TypeVar('T')
logger = logging.getLogger(__name__)


def handle_errors(
    error_types: Union[Type[Exception], List[Type[Exception]]] = Exception,
    default_return: Any = None,
    reraise_as: Optional[Type[NLWebError]] = None,
    log_level: str = "error",
    include_traceback: bool = True,
) -> Callable:
    """
    Decorator for standardized error handling.

    Args:
        error_types: Exception type(s) to catch
        default_return: Value to return on error (if not reraising)
        reraise_as: NLWebError subclass to reraise as
        log_level: Logging level ("debug", "info", "warning", "error")
        include_traceback: Whether to include traceback in logs

    Example:
        @handle_errors(error_types=ValueError, default_return=[])
        def parse_items(data):
            ...
    """
    if isinstance(error_types, type):
        error_types = [error_types]

    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @wraps(func)
        def sync_wrapper(*args, **kwargs) -> T:
            try:
                return func(*args, **kwargs)
            except tuple(error_types) as e:
                _handle_exception(func, e, log_level, include_traceback, reraise_as)
                return default_return

        @wraps(func)
        async def async_wrapper(*args, **kwargs) -> T:
            try:
                return await func(*args, **kwargs)
            except tuple(error_types) as e:
                _handle_exception(func, e, log_level, include_traceback, reraise_as)
                return default_return

        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        return sync_wrapper

    return decorator


def _handle_exception(
    func: Callable,
    error: Exception,
    log_level: str,
    include_traceback: bool,
    reraise_as: Optional[Type[NLWebError]],
) -> None:
    """Handle exception with logging and optional reraising."""
    log_func = getattr(logger, log_level, logger.error)

    message = f"Error in {func.__module__}.{func.__name__}: {error}"

    if include_traceback:
        log_func(message, exc_info=True)
    else:
        log_func(message)

    if reraise_as:
        raise reraise_as(
            message=str(error),
            cause=error,
            details={"function": func.__name__}
        ) from error


def handle_llm_errors(
    default_return: Any = None,
    reraise: bool = False,
    timeout_seconds: Optional[float] = None,
) -> Callable:
    """
    Decorator for handling LLM API errors.

    Args:
        default_return: Value to return on error
        reraise: Whether to reraise as LLMError
        timeout_seconds: Optional timeout for async calls

    Example:
        @handle_llm_errors(reraise=True)
        async def call_openai(prompt):
            ...
    """
    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @wraps(func)
        async def wrapper(*args, **kwargs) -> T:
            try:
                if timeout_seconds:
                    return await asyncio.wait_for(
                        func(*args, **kwargs),
                        timeout=timeout_seconds
                    )
                return await func(*args, **kwargs)

            except asyncio.TimeoutError as e:
                model = kwargs.get('model', 'unknown')
                error_msg = f"LLM timeout in {func.__name__}"
                logger.error(error_msg)

                if reraise:
                    raise LLMTimeoutError(
                        message=error_msg,
                        model=model,
                        cause=e,
                    ) from e
                return default_return

            except Exception as e:
                error_str = str(e).lower()
                model = kwargs.get('model', 'unknown')

                # Check for rate limiting
                if 'rate' in error_str and 'limit' in error_str:
                    logger.warning(f"LLM rate limit hit: {e}")
                    if reraise:
                        raise LLMRateLimitError(
                            message=f"Rate limit exceeded for {model}",
                            model=model,
                            cause=e,
                        ) from e
                    return default_return

                # General LLM error
                error_msg = f"LLM call failed in {func.__name__}: {e}"
                logger.error(error_msg, exc_info=True)

                if reraise:
                    raise LLMError(
                        message=error_msg,
                        model=model,
                        cause=e,
                    ) from e
                return default_return

        return wrapper
    return decorator


def handle_reasoning_errors(
    fallback_response: Optional[Callable] = None,
) -> Callable:
    """
    Decorator for handling reasoning pipeline errors.

    Args:
        fallback_response: Optional function to generate fallback response.
                           Called with (self, error) arguments.

    Example:
        @handle_reasoning_errors(fallback_response=lambda self, e: self._error_result(e))
        async def run_research(self, query):
            ...
    """
    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @wraps(func)
        async def wrapper(self, *args, **kwargs) -> T:
            try:
                return await func(self, *args, **kwargs)

            except NLWebError:
                # Re-raise our own exceptions (already handled)
                raise

            except Exception as e:
                logger.error(
                    f"Reasoning error in {func.__name__}: {e}",
                    exc_info=True
                )

                if fallback_response:
                    return fallback_response(self, error=e)

                # Default error response
                return [{
                    "@type": "Item",
                    "url": "internal://error",
                    "name": "System Error",
                    "site": "System",
                    "score": 0,
                    "description": f"推理系統發生錯誤: {type(e).__name__}",
                }]

        return wrapper
    return decorator


class ErrorContext:
    """Context manager for error handling with cleanup."""

    def __init__(
        self,
        operation: str,
        reraise_as: Optional[Type[NLWebError]] = None,
        cleanup: Optional[Callable] = None,
    ):
        """
        Initialize error context.

        Args:
            operation: Description of the operation
            reraise_as: Exception type to reraise as
            cleanup: Optional cleanup function to call on error
        """
        self.operation = operation
        self.reraise_as = reraise_as
        self.cleanup = cleanup

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_val is not None:
            logger.error(f"Error in {self.operation}: {exc_val}", exc_info=True)

            if self.cleanup:
                try:
                    self.cleanup()
                except Exception as cleanup_error:
                    logger.warning(f"Cleanup failed: {cleanup_error}")

            if self.reraise_as and not isinstance(exc_val, NLWebError):
                raise self.reraise_as(
                    message=f"{self.operation} failed: {exc_val}",
                    cause=exc_val,
                ) from exc_val

        return False  # Don't suppress the exception


def create_error_response(
    error: Union[Exception, str],
    query: Optional[str] = None,
    operation: Optional[str] = None,
) -> dict:
    """
    Create a standardized error response dict.

    Args:
        error: Exception or error message
        query: Optional query that caused the error
        operation: Optional operation name

    Returns:
        Error response dictionary
    """
    if isinstance(error, NLWebError):
        error_type = error.__class__.__name__
        message = error.message
        details = error.details
    else:
        error_type = type(error).__name__ if isinstance(error, Exception) else "Error"
        message = str(error)
        details = {}

    description = f"**{error_type}**\n\n{message}"

    if query:
        description += f"\n\n**Query**: {query}"

    if operation:
        description += f"\n\n**Operation**: {operation}"

    if details:
        description += "\n\n**Details**:\n"
        for key, value in details.items():
            description += f"- {key}: {value}\n"

    return {
        "@type": "Item",
        "url": "internal://error",
        "name": f"Error: {error_type}",
        "site": "System",
        "siteUrl": "internal",
        "score": 0,
        "description": description,
    }
```

---

### Step 3: Apply to Key Files

#### Update `orchestrator.py`

```python
# Add imports
from core.exceptions import (
    ReasoningError, AnalystError, CriticError,
    NoValidSourcesError, LLMError
)
from core.error_handling import (
    handle_reasoning_errors, handle_llm_errors,
    ErrorContext, create_error_response
)


class ResearchOrchestrator:

    @handle_reasoning_errors()
    async def run_research(self, ...) -> List[Dict[str, Any]]:
        """Execute deep research with error handling."""
        ...

    @handle_llm_errors(reraise=True, timeout_seconds=120)
    async def _call_analyst(self, prompt: str) -> str:
        """Call analyst with error handling."""
        ...

    async def _filter_and_prepare_sources(self, items, mode, tracer):
        """Filter sources with specific error handling."""
        filtered = self.source_filter.filter_and_enrich(items, mode)

        if not filtered:
            raise NoValidSourcesError(
                mode=mode,
                original_count=len(items),
                filtered_count=0,
            )

        return filtered
```

#### Update `analyst.py`

```python
from core.exceptions import AnalystError, LLMResponseParseError
from core.error_handling import handle_llm_errors, ErrorContext


class AnalystAgent:

    @handle_llm_errors(reraise=True)
    async def generate(self, prompt: str) -> str:
        """Generate with LLM error handling."""
        ...

    def _parse_response(self, response: str) -> Dict[str, Any]:
        """Parse response with specific error."""
        try:
            return json.loads(response)
        except json.JSONDecodeError as e:
            raise LLMResponseParseError(
                message="Failed to parse analyst response as JSON",
                raw_response=response,
                cause=e,
            )
```

---

## Testing

```python
# tests/test_error_handling.py

import pytest
from core.exceptions import (
    NLWebError, LLMError, NoValidSourcesError, LLMResponseParseError
)
from core.error_handling import (
    handle_errors, handle_llm_errors, create_error_response
)


class TestExceptions:
    """Test custom exceptions."""

    def test_nlweb_error_to_dict(self):
        """Test exception serialization."""
        error = NLWebError(
            message="Test error",
            details={"key": "value"},
        )

        result = error.to_dict()

        assert result["error_type"] == "NLWebError"
        assert result["message"] == "Test error"
        assert result["details"]["key"] == "value"

    def test_llm_error_with_model(self):
        """Test LLM error with model info."""
        error = LLMError(
            message="API failed",
            model="gpt-4",
            provider="openai",
        )

        assert error.details["model"] == "gpt-4"
        assert error.details["provider"] == "openai"

    def test_no_valid_sources_error(self):
        """Test source error with counts."""
        error = NoValidSourcesError(
            mode="strict",
            original_count=10,
            filtered_count=0,
        )

        assert "strict" in str(error)
        assert error.details["original_count"] == 10


class TestDecorators:
    """Test error handling decorators."""

    def test_handle_errors_catches(self):
        """Test decorator catches specified errors."""
        @handle_errors(error_types=ValueError, default_return="default")
        def failing_func():
            raise ValueError("test")

        result = failing_func()
        assert result == "default"

    def test_handle_errors_passes_through(self):
        """Test decorator passes through unmatched errors."""
        @handle_errors(error_types=ValueError, default_return="default")
        def failing_func():
            raise TypeError("test")

        with pytest.raises(TypeError):
            failing_func()

    @pytest.mark.asyncio
    async def test_handle_llm_errors_timeout(self):
        """Test LLM timeout handling."""
        @handle_llm_errors(reraise=True, timeout_seconds=0.1)
        async def slow_func():
            await asyncio.sleep(1)
            return "done"

        with pytest.raises(LLMTimeoutError):
            await slow_func()


class TestErrorResponse:
    """Test error response creation."""

    def test_create_from_exception(self):
        """Test creating response from exception."""
        error = LLMError("API failed", model="gpt-4")

        response = create_error_response(error, query="test query")

        assert response["@type"] == "Item"
        assert "LLMError" in response["name"]
        assert "API failed" in response["description"]
        assert "test query" in response["description"]

    def test_create_from_string(self):
        """Test creating response from string."""
        response = create_error_response("Something went wrong")

        assert "Something went wrong" in response["description"]
```

---

## Benefits

- ✅ Consistent error types across codebase
- ✅ Rich error context for debugging
- ✅ Decorators reduce boilerplate
- ✅ Serializable errors for API responses
- ✅ Clear error hierarchy
- ✅ Centralized error logging
