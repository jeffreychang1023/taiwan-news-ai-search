# P1.2: Refactor Orchestrator Monster Method - Detailed Implementation

**Estimated Effort**: 1-2 days
**Impact**: Critical - Makes 663-line method testable and maintainable
**ROI**: ⭐⭐⭐⭐⭐

---

## Problem Statement

The `run_research()` method in `orchestrator.py` is 663 lines long (lines 234-896), making it:
- **Impossible to test** individual phases in isolation
- **Hard to debug** when issues occur in specific stages
- **Difficult to modify** without breaking other parts
- **Violates Single Responsibility Principle** - does 7+ different things

### Current Structure Analysis

```python
async def run_research(...) -> List[Dict[str, Any]]:
    # Lines 234-896 (663 lines total)

    # 1. Setup & Initialization (lines 234-283) - 50 lines
    #    - Initialize logger, tracer
    #    - Log start event

    # 2. Source Filtering (lines 284-327) - 44 lines
    #    - Filter by source tier
    #    - Handle empty results

    # 3. Context Preparation (lines 328-360) - 33 lines
    #    - Format context with citations
    #    - Create source map

    # 4. Actor-Critic Loop (lines 361-685) - 325 lines!!!
    #    - Analyst phase (lines 370-421) - 52 lines
    #    - Gap detection (lines 422-526) - 105 lines
    #    - Critic review (lines 527-640) - 114 lines
    #    - Iteration control (lines 641-685) - 45 lines

    # 5. Final Report Generation (lines 686-796) - 111 lines
    #    - Writer composition
    #    - Format result

    # 6. Cleanup & Return (lines 797-896) - 100 lines
    #    - Save logs
    #    - Return formatted result
```

---

## Solution Architecture

Break the monster method into focused, testable methods:

```
run_research()                          # Orchestration only (80 lines)
├── _setup_research_session()           # Logging & tracing setup
├── _filter_and_prepare_sources()       # Source tier filtering
├── _format_research_context()          # Context formatting
├── _run_actor_critic_loop()            # Main iteration logic
│   ├── _run_analyst_phase()            # Analyst execution
│   ├── _run_critic_phase()             # Critic review
│   ├── _handle_gap_enrichment()        # Gap detection
│   │   └── _execute_gap_searches()     # Secondary searches
│   └── _should_stop_iteration()        # Convergence check
├── _generate_final_report()            # Writer composition
└── _create_error_response()            # Error handling
```

---

## Implementation

### Complete Refactored Code

All new methods to add to `ResearchOrchestrator` class:

```python
# Add these methods to code/python/reasoning/orchestrator.py

def _setup_research_session(
    self,
    query_id: str,
    query: str,
    mode: str,
) -> Tuple['IterationLogger', Optional['ConsoleTracer']]:
    """
    Initialize logging and tracing for research session.

    Args:
        query_id: Unique query identifier
        query: User's research question
        mode: Research mode (strict, discovery, monitor)

    Returns:
        Tuple of (iteration_logger, tracer)
    """
    # Initialize iteration logger
    iteration_logger = IterationLogger(query_id)

    # Initialize console tracer (if enabled)
    tracer = None
    tracing_config = CONFIG.reasoning_params.get("tracing", {})

    if tracing_config.get("console", {}).get("enabled", True):
        import os
        verbosity = os.getenv("REASONING_TRACE_LEVEL") or \
                    tracing_config.get("console", {}).get("level", "DEBUG")
        from reasoning.utils.console_tracer import ConsoleTracer
        tracer = ConsoleTracer(query_id=query_id, verbosity=verbosity)
        tracer.start_research(query=query, mode=mode, items=[])

    self.logger.info(
        f"Starting deep research: query='{query}', mode={mode}, "
        f"query_id={query_id}"
    )

    return iteration_logger, tracer


async def _filter_and_prepare_sources(
    self,
    items: List[Dict[str, Any]],
    mode: str,
    tracer: Optional['ConsoleTracer'],
) -> List[Dict[str, Any]]:
    """
    Apply source tier filtering based on research mode.

    Args:
        items: Raw retrieved items
        mode: Research mode
        tracer: Optional tracer for logging

    Returns:
        Filtered items list

    Raises:
        ValueError: If no sources remain after filtering
    """
    self.logger.info(f"Filtering {len(items)} sources for mode={mode}")

    filtered_items = self.source_filter.filter_and_enrich(items, mode)

    if tracer:
        tracer.source_filtering(
            original_items=items,
            filtered_items=filtered_items,
            mode=mode
        )

    self.logger.info(
        f"Source filtering complete: {len(filtered_items)} sources "
        f"(from {len(items)} original)"
    )

    if not filtered_items:
        raise ValueError(
            f"No sources available after filtering. "
            f"Original: {len(items)}, Filtered: 0, Mode: {mode}"
        )

    return filtered_items


async def _format_research_context(
    self,
    items: List[Dict[str, Any]],
    tracer: Optional['ConsoleTracer'],
) -> Tuple[str, Dict[int, Dict[str, Any]]]:
    """
    Format items into citation context.

    Args:
        items: Filtered source items
        tracer: Optional tracer

    Returns:
        Tuple of (formatted_context_string, source_id_map)
    """
    formatted_context, source_map = self._format_context_shared(items)

    if tracer:
        tracer.context_formatted(
            source_map=source_map,
            formatted_context=formatted_context
        )

    self.logger.debug(f"Context formatted: {len(source_map)} sources mapped")

    return formatted_context, source_map


async def _run_actor_critic_loop(
    self,
    query: str,
    mode: str,
    formatted_context: str,
    source_map: Dict[int, Dict[str, Any]],
    temporal_context: Optional[Dict[str, Any]],
    enable_web_search: bool,
    iteration_logger: 'IterationLogger',
    tracer: Optional['ConsoleTracer'],
) -> Tuple[Optional[str], Optional[str], List[Dict[str, Any]]]:
    """
    Run the iterative Actor-Critic research loop.

    Args:
        query: User's research question
        mode: Research mode
        formatted_context: Formatted citation context
        source_map: Source ID to item mapping
        temporal_context: Optional temporal constraints
        enable_web_search: Whether web search is enabled
        iteration_logger: Logger for iteration tracking
        tracer: Optional console tracer

    Returns:
        Tuple of (final_draft, final_review, all_sources_used)
    """
    max_iterations = CONFIG.reasoning_params.get("max_iterations", 3)

    draft = None
    review = None
    current_sources = list(source_map.values())

    for iteration in range(max_iterations):
        self.logger.info(f"Starting iteration {iteration + 1}/{max_iterations}")

        # Analyst phase
        draft = await self._run_analyst_phase(
            query=query,
            mode=mode,
            formatted_context=formatted_context,
            temporal_context=temporal_context,
            enable_web_search=enable_web_search,
            iteration=iteration,
            previous_review=review,
            tracer=tracer,
        )

        iteration_logger.log_iteration(iteration, "analyst", draft)

        # Gap detection & enrichment (not on last iteration)
        if iteration < max_iterations - 1:
            new_sources = await self._handle_gap_enrichment(
                draft=draft,
                mode=mode,
                enable_web_search=enable_web_search,
                tracer=tracer,
            )

            if new_sources:
                current_sources.extend(new_sources)
                formatted_context, source_map = self._format_research_context(
                    current_sources,
                    tracer=None
                )
                self.logger.info(
                    f"Context enriched: {len(new_sources)} new sources added"
                )

        # Critic phase
        review = await self._run_critic_phase(
            draft=draft,
            query=query,
            mode=mode,
            formatted_context=formatted_context,
            temporal_context=temporal_context,
            iteration=iteration,
            tracer=tracer,
        )

        iteration_logger.log_iteration(iteration, "critic", review)

        # Check convergence
        if self._should_stop_iteration(review, iteration, max_iterations):
            self.logger.info(
                f"Convergence achieved at iteration {iteration + 1}"
            )
            break

    return draft, review, current_sources


async def _run_analyst_phase(
    self,
    query: str,
    mode: str,
    formatted_context: str,
    temporal_context: Optional[Dict[str, Any]],
    enable_web_search: bool,
    iteration: int,
    previous_review: Optional[str],
    tracer: Optional['ConsoleTracer'],
) -> str:
    """Execute analyst research phase."""
    self.logger.debug(f"Running analyst phase (iteration {iteration})")

    if tracer:
        tracer.log_event("analyst_start", {"iteration": iteration})

    prompt = self.analyst._build_research_prompt(
        query=query,
        formatted_context=formatted_context,
        mode=mode,
        temporal_context=temporal_context,
        enable_argument_graph=CONFIG.reasoning_params.get("enable_argument_graph", False),
        enable_knowledge_graph=CONFIG.reasoning_params.get("enable_knowledge_graph", False),
        enable_gap_enrichment=CONFIG.reasoning_params.get("enable_gap_enrichment", False),
        enable_web_search=enable_web_search,
    )

    if iteration > 0 and previous_review:
        prompt += f"\n\n## 前次審查反饋\n\n{previous_review}\n\n請根據以上反饋改進你的分析。"

    draft = await self.analyst.generate(prompt)

    if tracer:
        tracer.log_event("analyst_complete", {
            "iteration": iteration,
            "draft_length": len(draft),
        })

    self.logger.debug(f"Analyst phase complete: {len(draft)} chars")

    return draft


async def _run_critic_phase(
    self,
    draft: str,
    query: str,
    mode: str,
    formatted_context: str,
    temporal_context: Optional[Dict[str, Any]],
    iteration: int,
    tracer: Optional['ConsoleTracer'],
) -> str:
    """Execute critic review phase."""
    self.logger.debug(f"Running critic phase (iteration {iteration})")

    if tracer:
        tracer.log_event("critic_start", {"iteration": iteration})

    prompt = self.critic._build_review_prompt(
        draft=draft,
        query=query,
        mode=mode,
        formatted_context=formatted_context,
        temporal_context=temporal_context,
    )

    review = await self.critic.generate(prompt)

    if tracer:
        tracer.log_event("critic_complete", {
            "iteration": iteration,
            "review_length": len(review),
        })

    self.logger.debug(f"Critic phase complete: {len(review)} chars")

    return review


def _should_stop_iteration(
    self,
    review: str,
    iteration: int,
    max_iterations: int,
) -> bool:
    """Check if Actor-Critic loop should stop."""
    if iteration >= max_iterations - 1:
        return True

    try:
        review_data = json.loads(review)
        status = review_data.get("status", "")

        if status == "APPROVED":
            return True
        elif status in ["REJECTED", "NEEDS_REVISION"]:
            return False
        else:
            self.logger.warning(f"Unknown critic status: {status}")
            return False

    except (json.JSONDecodeError, AttributeError) as e:
        self.logger.warning(f"Failed to parse critic review: {e}")
        return False


async def _handle_gap_enrichment(
    self,
    draft: str,
    mode: str,
    enable_web_search: bool,
    tracer: Optional['ConsoleTracer'],
) -> List[Dict[str, Any]]:
    """Detect information gaps and enrich context with additional searches."""
    if not enable_web_search:
        return []

    try:
        draft_data = json.loads(draft)
        gaps = draft_data.get("information_gaps", [])
    except json.JSONDecodeError as e:
        self.logger.warning(f"Failed to parse draft for gaps: {e}")
        return []

    if not gaps:
        self.logger.debug("No information gaps detected")
        return []

    search_gaps = [
        g for g in gaps
        if g.get("importance") in ["critical", "high"]
        and g.get("suggested_search_queries")
    ]

    if not search_gaps:
        self.logger.debug(
            f"Found {len(gaps)} gaps, but none require immediate search"
        )
        return []

    self.logger.info(
        f"Gap enrichment: {len(search_gaps)} gaps require searching"
    )

    if tracer:
        tracer.log_event("gap_detection_start", {
            "gap_count": len(search_gaps)
        })

    new_sources = await self._execute_gap_searches(
        gaps=search_gaps,
        mode=mode,
    )

    if tracer:
        tracer.log_event("gap_detection_complete", {
            "new_sources": len(new_sources)
        })

    self.logger.info(
        f"Gap enrichment complete: {len(new_sources)} new sources found"
    )

    return new_sources


async def _execute_gap_searches(
    self,
    gaps: List[Dict[str, Any]],
    mode: str,
) -> List[Dict[str, Any]]:
    """Execute searches for identified information gaps."""
    new_sources = []

    for gap in gaps[:3]:
        gap_id = gap.get("gap_id", "unknown")
        queries = gap.get("suggested_search_queries", [])

        for query in queries[:2]:
            try:
                self.logger.debug(f"Gap search: {query} (from {gap_id})")
                results = await self._perform_web_search(query, mode)

                if results:
                    new_sources.extend(results)
                    self.logger.debug(
                        f"Gap search succeeded: {len(results)} results"
                    )
                else:
                    self.logger.debug("Gap search returned no results")

            except Exception as e:
                self.logger.warning(
                    f"Gap search failed for '{query}': {e}",
                    exc_info=True
                )
                continue

    # Remove duplicates by URL
    seen_urls = set()
    unique_sources = []

    for source in new_sources:
        url = source.get("url", "")
        if url and url not in seen_urls:
            seen_urls.add(url)
            unique_sources.append(source)

    return unique_sources


async def _perform_web_search(
    self,
    query: str,
    mode: str,
) -> List[Dict[str, Any]]:
    """Perform web search via appropriate handler."""
    if not hasattr(self, 'web_search_handler'):
        self.logger.warning("Web search handler not configured")
        return []

    try:
        results = await self.web_search_handler.search(
            query=query,
            max_results=5,
            mode=mode,
        )
        return results
    except Exception as e:
        self.logger.error(f"Web search error: {e}")
        return []


async def _generate_final_report(
    self,
    draft: str,
    review: Optional[str],
    query: str,
    mode: str,
    all_sources: List[Dict[str, Any]],
    tracer: Optional['ConsoleTracer'],
) -> str:
    """Generate final formatted report using Writer agent."""
    self.logger.debug("Generating final report with Writer agent")

    if tracer:
        tracer.log_event("writer_start", {})

    prompt = self.writer._build_compose_prompt(
        draft=draft,
        review=review,
        query=query,
        mode=mode,
    )

    final_report = await self.writer.generate(prompt)

    if tracer:
        tracer.log_event("writer_complete", {
            "report_length": len(final_report)
        })

    self.logger.info(f"Final report generated: {len(final_report)} chars")

    return final_report


def _create_error_response(
    self,
    error_msg: str,
    query: str,
    details: Optional[Dict[str, Any]] = None,
) -> List[Dict[str, Any]]:
    """Create standardized error response."""
    self.logger.error(f"Research error: {error_msg}")

    if details:
        self.logger.debug(f"Error details: {details}")

    description = f"**Deep Research 無法完成**\n\n{error_msg}"

    if details:
        description += "\n\n**詳細資訊**:\n"
        for key, value in details.items():
            description += f"- {key}: {value}\n"

    return [{
        "@type": "Item",
        "url": "internal://error",
        "name": "Deep Research 錯誤",
        "site": "系統訊息",
        "siteUrl": "internal",
        "score": 0,
        "description": description,
    }]


# REPLACE the existing run_research method (lines 234-896) with this:

async def run_research(
    self,
    query: str,
    mode: str,
    items: List[Dict[str, Any]],
    temporal_context: Optional[Dict[str, Any]] = None,
    enable_kg: bool = False,
    enable_web_search: bool = False
) -> List[Dict[str, Any]]:
    """
    Execute deep research using Actor-Critic loop.

    Orchestrates the multi-agent reasoning workflow:
    1. Setup logging and tracing
    2. Filter sources by tier
    3. Format context with citations
    4. Run Actor-Critic iteration loop
    5. Generate final report

    Args:
        query: User's research question
        mode: Research mode (strict, discovery, monitor)
        items: Retrieved items from search
        temporal_context: Optional temporal information
        enable_kg: Enable knowledge graph generation
        enable_web_search: Enable web search for dynamic data

    Returns:
        List of NLWeb Item dicts compatible with create_assistant_result()

    Raises:
        ValueError: If no valid sources after filtering
    """
    query_id = getattr(self.handler, 'query_id', f'reasoning_{hash(query)}')

    # Setup session
    iteration_logger, tracer = self._setup_research_session(
        query_id=query_id,
        query=query,
        mode=mode,
    )

    try:
        # Filter sources
        filtered_items = await self._filter_and_prepare_sources(
            items=items,
            mode=mode,
            tracer=tracer,
        )

        # Format context
        formatted_context, source_map = await self._format_research_context(
            items=filtered_items,
            tracer=tracer,
        )

        # Run Actor-Critic loop
        draft, review, all_sources = await self._run_actor_critic_loop(
            query=query,
            mode=mode,
            formatted_context=formatted_context,
            source_map=source_map,
            temporal_context=temporal_context,
            enable_web_search=enable_web_search,
            iteration_logger=iteration_logger,
            tracer=tracer,
        )

        if not draft:
            return self._create_error_response(
                error_msg="分析師未產生任何分析結果",
                query=query,
                details={"mode": mode, "sources": len(all_sources)}
            )

        # Generate final report
        final_report = await self._generate_final_report(
            draft=draft,
            review=review,
            query=query,
            mode=mode,
            all_sources=all_sources,
            tracer=tracer,
        )

        # Save iteration log
        iteration_logger.save()

        if tracer:
            tracer.log_event("research_complete", {})

        # Format response
        return [{
            "@type": "Item",
            "url": f"research://{query_id}",
            "name": f"Deep Research: {query}",
            "site": "Deep Research System",
            "siteUrl": "internal://reasoning",
            "score": 1.0,
            "description": final_report,
        }]

    except ValueError as e:
        return self._create_error_response(
            error_msg=str(e),
            query=query,
            details={"original_items": len(items), "mode": mode}
        )

    except Exception as e:
        self.logger.error(f"Unexpected error in run_research: {e}", exc_info=True)

        return self._create_error_response(
            error_msg=f"系統錯誤: {type(e).__name__}",
            query=query,
            details={"error": str(e)}
        )
```

---

## Complete Diff Summary

```diff
Changes to code/python/reasoning/orchestrator.py:

Lines 234-896 (663 lines) REPLACED with:
- New run_research() method (~80 lines)

Added new methods (total ~400 lines):
+ _setup_research_session()
+ _filter_and_prepare_sources()
+ _format_research_context()
+ _run_actor_critic_loop()
+ _run_analyst_phase()
+ _run_critic_phase()
+ _should_stop_iteration()
+ _handle_gap_enrichment()
+ _execute_gap_searches()
+ _perform_web_search()
+ _generate_final_report()
+ _create_error_response()

Net change: -183 lines (663 → 480)
Readability: Dramatically improved
Testability: Each method now independently testable
```

---

## Testing Strategy

### Unit Tests (tests/test_orchestrator_refactor.py)

See P1.2 document for complete test suite. Key tests:
- Test each extracted method in isolation
- Test error handling paths
- Test iteration convergence logic
- Test gap enrichment flow

### Integration Tests

Compare outputs before/after refactoring to ensure no behavioral changes.

---

## Implementation Checklist

- [ ] Backup orchestrator.py
- [ ] Add all 12 new helper methods
- [ ] Replace run_research() implementation
- [ ] Add type hints throughout
- [ ] Write unit tests (achieve >80% coverage)
- [ ] Run integration tests
- [ ] Compare outputs with original
- [ ] Performance benchmarking
- [ ] Code review
- [ ] Merge to main

---

## Benefits

- ✅ Main method: 663 lines → 80 lines
- ✅ Each phase independently testable
- ✅ Clear separation of concerns
- ✅ Easier debugging with focused methods
- ✅ Better error handling per phase
- ✅ Improved code review experience

---

## Next: P1.3 Simplify Deep Research
